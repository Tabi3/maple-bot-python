from Maple_bot_functions.ExternalLibraries import *

nest_asyncio.apply()


class _MyAHS(AsyncHTMLSession):
    def run(self, coros):
        try:
            loop = asyncio.get_event_loop()
            tasks = [asyncio.ensure_future(coro) for coro in coros]
            done, _ = loop.run_until_complete(asyncio.wait(tasks))
            return [t.result() for t in done]
        except RuntimeError:
            loop = asyncio.new_event_loop()
            tasks = [asyncio.ensure_future(coro, loop=loop) for coro in coros]
            done, _ = loop.run_until_complete(asyncio.wait(tasks))
            return [t.result() for t in done]


class MapleSession:

    def __init__(self, AHS: _MyAHS, url: str = None, urls: list = None) -> None:
        self.url, self.urls, self.AHS = url, urls, AHS

    def close(self):
        asyncio.run(self.AHS.close())

    async def _render_site(self, sleep: int = 10, keep_page: bool = True, _url=None, wait: int = 5):
        dom = await self.AHS.get(_url or self.url)
        while True:
            try:
                await dom.html.arender(sleep=sleep, keep_page=keep_page, wait=wait)
                break
            except Exception:
                ...
        return BeautifulSoup(dom.html.html, "lxml")

    def render_sites(self, sleep: int = 10, keep_page: bool = True, wait: int = 5) -> list:
        return self.AHS.run(self._render_site(sleep, keep_page, i, wait) for i in iter(self.urls))

    def render_site(self, sleep: int = 10, keep_page: bool = True, wait: int = 5) -> list:
        return self.AHS.run([self._render_site(sleep, keep_page, self.url, wait)])[0]


def WolframQuery(query: str) -> dict:
    session = MapleSession(AHS=_MyAHS(), url=(url := "https://www.wolframalpha.com/input?i=" +
                                              query.translate(str.maketrans({'+': '%2B', ' ': '+'}))))
    r = (session).render_site(10)
    def src_alt(i, j): return j["alt"] == i.find('h2', {"class": "_3OwK"}).get_text() if i.find(
        'h2', {"class": "_3OwK"}) else False,  "_3WIS" in i["class"] and "_3c8e" in j["class"]
    scope = {
        "myVar": [
            j["src"]
            if src_alt(i, j)[0] else j["alt"]
            if src_alt(i, j)[1] else f"{j['alt']} |Child|"
            for i in r.find_all("section", {"class": "_gtUC"}) for j in i.find_all("img")
        ],
        "keys": (n := [j.get_text() for j in r.find_all("h2", {"class": "_3OwK"})] + ['_']),
        "myDict": {i: [] for i in n},
        "counter": -1,
        "_": None
    }
    [
        scope["myDict"][scope["keys"][scope["counter"]]].append(i[:-8] if i[:-8] != scope["keys"][scope["counter"]] else "data:")
        if i.endswith("|Child|") else scope.update(
            _=scope["myDict"][scope["keys"][scope["counter"] + 1]].append(i if i != scope["keys"][scope["counter"]] else "data:"),
            counter=scope["counter"] + 1
        ) for i in scope["myVar"]
    ]
    session.close()
    return [scope["myDict"]][0], url


def get_video(info: dict) -> dict:
    soup = BeautifulSoup(requests.get(info[list(info.keys())[0]][0]).text, 'lxml')
    return {"title": list(info.keys())[0],
            "description": soup.find('div', {"class": "summary__content"}).get_text().strip(),
            "episodes": [i['href'] for i in soup.find('ul', {"class": ["main", "version-chap"]}).find_all('a')],
            "tags": list(filter(lambda x: x not in [' ', '\n'],
                                [i.get_text() for i in soup.find('div', {"class": "wp-manga-tags-list"})])),
            "url/image": info[list(info.keys())[0]]}


def HentaiHaven() -> dict:
    return {j['title']: [j['href'], j.contents[1].attrs[('data-src' if 'data-src' in j.contents[1].attrs else 'src')].replace(' ', '%20')] for i in BeautifulSoup(requests.get("https://hentaihaven.xxx/").text, 'lxml').find_all("div", {"class": ["page-item-detail", "video"]}) for j in i.find_all('a') if 'title' in j.attrs and 'episode' not in j['href']}


def get_by_tags(tags: str = "straight", straight: bool = True, page: int = 0) -> list:
    x = '+'.join(tags.split('|'))
    images = requests.get((f"https://rule34.xxx/index.php?page=post&s=list&tags={x}"
                          + (f"+straight&pid={page*25}" if x else f"straight+&pid={page*25}")) if straight else
                          f"https://rule34.xxx/index.php?page=post&s=list&tags={x}&pid={page*25}")
    print((f"https://rule34.xxx/index.php?page=post&s=list&tags={x}"
           + (f"+straight&pid={page*25}" if x else f"straight+&pid={page*25}")) if straight else
          f"https://rule34.xxx/index.php?page=post&s=list&tags={x}&pid={page*25}")

    soup = BeautifulSoup(images.text, 'lxml')
    return [["https://rule34.xxx/"+i.find('a')["href"], i.find('img')["alt"][:97]+'...'] for i in soup.find_all('span', {"class": "thumb"})][:25]


def rule34_by_url(url: str):
    soup = BeautifulSoup(requests.get(url).text, 'lxml')
    _tags = [[i['class'][0], i.find('a').get_text()] for i in soup.find('ul', id="tag-sidebar").find_all('li', {"class": "tag"})]
    tags = (lambda tags: ([tags[i[0]].append(f"`{i[1]}`") for i in _tags]+[tags]))({i[0]: [] for i in _tags})[-1]
    image = soup.find('a', style="font-weight: bold;")["href"]
    return tags, image


print(rule34_by_url(get_by_tags()[3][0])) if __name__ == "__main__" else ...
